{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarcasm Detection With Naive Bayes Classifier\n",
    "Author: Nam Bui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typing\n",
    "from __future__ import annotations\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Pre-processing functions\n",
    "from common import *\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nbui2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "try:\n",
    "    nltk.find(\"tokenizers/punkt/english.pickle\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "try:\n",
    "    nltk.find(\"corpora/stopwords\")\n",
    "except:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "try:\n",
    "    nltk.find(\"corpora/wordnet\")\n",
    "except:\n",
    "    nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>storybook romance leads to in-flight-magazine ...</td>\n",
       "      <td>https://local.theonion.com/storybook-romance-l...</td>\n",
       "      <td>[&lt;s&gt;, storybook, romance, leads, to, in, &lt;HYPH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nation exhibits strange preoccupation with man...</td>\n",
       "      <td>https://www.theonion.com/nation-exhibits-stran...</td>\n",
       "      <td>[&lt;s&gt;, nation, exhibits, strange, preoccupation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20 indoor wall planters to take your houseplan...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-best-...</td>\n",
       "      <td>[&lt;s&gt;, 20, indoor, wall, planters, to, take, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>prima donna species just has to have every par...</td>\n",
       "      <td>https://www.theonion.com/prima-donna-species-j...</td>\n",
       "      <td>[&lt;s&gt;, prima, donna, species, just, has, to, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>turkish soccer body penalizes kurdish club ami...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/turkish-s...</td>\n",
       "      <td>[&lt;s&gt;, turkish, soccer, body, penalizes, kurdis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  storybook romance leads to in-flight-magazine ...   \n",
       "1             1  nation exhibits strange preoccupation with man...   \n",
       "2             0  20 indoor wall planters to take your houseplan...   \n",
       "3             1  prima donna species just has to have every par...   \n",
       "4             0  turkish soccer body penalizes kurdish club ami...   \n",
       "\n",
       "                                        article_link  \\\n",
       "0  https://local.theonion.com/storybook-romance-l...   \n",
       "1  https://www.theonion.com/nation-exhibits-stran...   \n",
       "2  https://www.huffingtonpost.com/entry/the-best-...   \n",
       "3  https://www.theonion.com/prima-donna-species-j...   \n",
       "4  https://www.huffingtonpost.com/entry/turkish-s...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [<s>, storybook, romance, leads, to, in, <HYPH...  \n",
       "1  [<s>, nation, exhibits, strange, preoccupation...  \n",
       "2  [<s>, 20, indoor, wall, planters, to, take, yo...  \n",
       "3  [<s>, prima, donna, species, just, has, to, ha...  \n",
       "4  [<s>, turkish, soccer, body, penalizes, kurdis...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_json(\"../data/train_tokenized.json\", lines=True)\n",
    "val_df = pd.read_json(\"../data/val_tokenized.json\", lines=True)\n",
    "test_df = pd.read_json(\"../data/test_tokenized.json\", lines=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>storybook romance leads to in-flight-magazine ...</td>\n",
       "      <td>https://local.theonion.com/storybook-romance-l...</td>\n",
       "      <td>[&lt;s&gt;, storybook, romance, leads, &lt;HYPH&gt;, fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nation exhibits strange preoccupation with man...</td>\n",
       "      <td>https://www.theonion.com/nation-exhibits-stran...</td>\n",
       "      <td>[&lt;s&gt;, nation, exhibits, strange, preoccupation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20 indoor wall planters to take your houseplan...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-best-...</td>\n",
       "      <td>[&lt;s&gt;, 20, indoor, wall, planters, take, housep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>prima donna species just has to have every par...</td>\n",
       "      <td>https://www.theonion.com/prima-donna-species-j...</td>\n",
       "      <td>[&lt;s&gt;, prima, donna, species, every, part, natu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>turkish soccer body penalizes kurdish club ami...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/turkish-s...</td>\n",
       "      <td>[&lt;s&gt;, turkish, soccer, body, penalizes, kurdis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  storybook romance leads to in-flight-magazine ...   \n",
       "1             1  nation exhibits strange preoccupation with man...   \n",
       "2             0  20 indoor wall planters to take your houseplan...   \n",
       "3             1  prima donna species just has to have every par...   \n",
       "4             0  turkish soccer body penalizes kurdish club ami...   \n",
       "\n",
       "                                        article_link  \\\n",
       "0  https://local.theonion.com/storybook-romance-l...   \n",
       "1  https://www.theonion.com/nation-exhibits-stran...   \n",
       "2  https://www.huffingtonpost.com/entry/the-best-...   \n",
       "3  https://www.theonion.com/prima-donna-species-j...   \n",
       "4  https://www.huffingtonpost.com/entry/turkish-s...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [<s>, storybook, romance, leads, <HYPH>, fligh...  \n",
       "1  [<s>, nation, exhibits, strange, preoccupation...  \n",
       "2  [<s>, 20, indoor, wall, planters, take, housep...  \n",
       "3  [<s>, prima, donna, species, every, part, natu...  \n",
       "4  [<s>, turkish, soccer, body, penalizes, kurdis...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(sentence: list[str]) -> list[str]:\n",
    "    return [w for w in sentence if w not in stop_words]\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(remove_stopwords)\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>storybook romance leads to in-flight-magazine ...</td>\n",
       "      <td>https://local.theonion.com/storybook-romance-l...</td>\n",
       "      <td>[&lt;s&gt;, storybook, romance, lead, &lt;HYPH&gt;, flight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nation exhibits strange preoccupation with man...</td>\n",
       "      <td>https://www.theonion.com/nation-exhibits-stran...</td>\n",
       "      <td>[&lt;s&gt;, nation, exhibit, strange, preoccupation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20 indoor wall planters to take your houseplan...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-best-...</td>\n",
       "      <td>[&lt;s&gt;, 20, indoor, wall, planter, take, housepl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>prima donna species just has to have every par...</td>\n",
       "      <td>https://www.theonion.com/prima-donna-species-j...</td>\n",
       "      <td>[&lt;s&gt;, prima, donna, specie, every, part, natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>turkish soccer body penalizes kurdish club ami...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/turkish-s...</td>\n",
       "      <td>[&lt;s&gt;, turkish, soccer, body, penalizes, kurdis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  storybook romance leads to in-flight-magazine ...   \n",
       "1             1  nation exhibits strange preoccupation with man...   \n",
       "2             0  20 indoor wall planters to take your houseplan...   \n",
       "3             1  prima donna species just has to have every par...   \n",
       "4             0  turkish soccer body penalizes kurdish club ami...   \n",
       "\n",
       "                                        article_link  \\\n",
       "0  https://local.theonion.com/storybook-romance-l...   \n",
       "1  https://www.theonion.com/nation-exhibits-stran...   \n",
       "2  https://www.huffingtonpost.com/entry/the-best-...   \n",
       "3  https://www.theonion.com/prima-donna-species-j...   \n",
       "4  https://www.huffingtonpost.com/entry/turkish-s...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [<s>, storybook, romance, lead, <HYPH>, flight...  \n",
       "1  [<s>, nation, exhibit, strange, preoccupation,...  \n",
       "2  [<s>, 20, indoor, wall, planter, take, housepl...  \n",
       "3  [<s>, prima, donna, specie, every, part, natur...  \n",
       "4  [<s>, turkish, soccer, body, penalizes, kurdis...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(sentence: list[str]) -> list[str]:\n",
    "    return [lemmatizer.lemmatize(w) for w in sentence]\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(lemmatize)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>storybook romance leads to in-flight-magazine ...</td>\n",
       "      <td>https://local.theonion.com/storybook-romance-l...</td>\n",
       "      <td>[storybook, romance, lead, &lt;HYPH&gt;, flight, &lt;HY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nation exhibits strange preoccupation with man...</td>\n",
       "      <td>https://www.theonion.com/nation-exhibits-stran...</td>\n",
       "      <td>[nation, exhibit, strange, preoccupation, mann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20 indoor wall planters to take your houseplan...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-best-...</td>\n",
       "      <td>[20, indoor, wall, planter, take, houseplant, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>prima donna species just has to have every par...</td>\n",
       "      <td>https://www.theonion.com/prima-donna-species-j...</td>\n",
       "      <td>[prima, donna, specie, every, part, natural, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>turkish soccer body penalizes kurdish club ami...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/turkish-s...</td>\n",
       "      <td>[turkish, soccer, body, penalizes, kurdish, cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  storybook romance leads to in-flight-magazine ...   \n",
       "1             1  nation exhibits strange preoccupation with man...   \n",
       "2             0  20 indoor wall planters to take your houseplan...   \n",
       "3             1  prima donna species just has to have every par...   \n",
       "4             0  turkish soccer body penalizes kurdish club ami...   \n",
       "\n",
       "                                        article_link  \\\n",
       "0  https://local.theonion.com/storybook-romance-l...   \n",
       "1  https://www.theonion.com/nation-exhibits-stran...   \n",
       "2  https://www.huffingtonpost.com/entry/the-best-...   \n",
       "3  https://www.theonion.com/prima-donna-species-j...   \n",
       "4  https://www.huffingtonpost.com/entry/turkish-s...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [storybook, romance, lead, <HYPH>, flight, <HY...  \n",
       "1  [nation, exhibit, strange, preoccupation, mann...  \n",
       "2  [20, indoor, wall, planter, take, houseplant, ...  \n",
       "3  [prima, donna, specie, every, part, natural, h...  \n",
       "4  [turkish, soccer, body, penalizes, kurdish, cl...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove start and end tokens\n",
    "# They're not useful for NB classification because they appear in every sample\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(lambda x: x[1:-1])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "def build_vocabulary(df) -> tuple[list[str], dict[str, int]]:\n",
    "    # Get raw counts\n",
    "    vocab = defaultdict(int)\n",
    "    for _, tk_list in train_df[\"tokens\"].items():\n",
    "        for tk in tk_list:\n",
    "            if tk not in [TK_HYPH, TK_UNK]:\n",
    "                vocab[tk] += 1\n",
    "\n",
    "    # Any word occurring less than 5 times becomes unknown\n",
    "    word_list = [TK_HYPH, TK_UNK]\n",
    "    word2ind = {word_list[i]: i for i in range(len(word_list))}\n",
    "    for word, count in vocab.items():\n",
    "        if count >= 5:\n",
    "            word2ind[word] = len(word_list)\n",
    "            word_list.append(word)\n",
    "    return word_list, word2ind\n",
    "\n",
    "vocab, word2ind = build_vocabulary(train_df)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>storybook romance leads to in-flight-magazine ...</td>\n",
       "      <td>https://local.theonion.com/storybook-romance-l...</td>\n",
       "      <td>[2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nation exhibits strange preoccupation with man...</td>\n",
       "      <td>https://www.theonion.com/nation-exhibits-stran...</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20 indoor wall planters to take your houseplan...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-best-...</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>prima donna species just has to have every par...</td>\n",
       "      <td>https://www.theonion.com/prima-donna-species-j...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>turkish soccer body penalizes kurdish club ami...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/turkish-s...</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  storybook romance leads to in-flight-magazine ...   \n",
       "1             1  nation exhibits strange preoccupation with man...   \n",
       "2             0  20 indoor wall planters to take your houseplan...   \n",
       "3             1  prima donna species just has to have every par...   \n",
       "4             0  turkish soccer body penalizes kurdish club ami...   \n",
       "\n",
       "                                        article_link  \\\n",
       "0  https://local.theonion.com/storybook-romance-l...   \n",
       "1  https://www.theonion.com/nation-exhibits-stran...   \n",
       "2  https://www.huffingtonpost.com/entry/the-best-...   \n",
       "3  https://www.theonion.com/prima-donna-species-j...   \n",
       "4  https://www.huffingtonpost.com/entry/turkish-s...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 3, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...  \n",
       "3  [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize tokens  \n",
    "def vectorize_tokens(sentence: list[str]) -> NDArray:\n",
    "    v = np.zeros(len(vocab), dtype=np.int8)\n",
    "    i_UNK = word2ind[TK_UNK]\n",
    "    for tk in sentence:\n",
    "        v[word2ind.get(tk, i_UNK)] += 1\n",
    "    return v\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(vectorize_tokens)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data\n",
    "train_X = np.vstack(train_df[\"tokens\"])\n",
    "train_Y = np.stack(train_df[\"is_sarcastic\"], dtype=np.int8)\n",
    "val_X = np.vstack(val_df[\"tokens\"], dtype=np.int8)\n",
    "val_Y = np.stack(val_df[\"is_sarcastic\"], dtype=np.int8)\n",
    "test_X = np.vstack(test_df[\"tokens\"], dtype=np.int8)\n",
    "test_Y = np.stack(test_df[\"is_sarcastic\"], dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_confusion_matrix(cm: NDArray) -> None:\n",
    "    # Calculate and print metrics based off confusion matrix\n",
    "    precision = (cm[0][0]) / (cm[0][0] + cm[1][0])\n",
    "    recall = (cm[0][0]) / (cm[0][0] + cm[0][1])\n",
    "    print(f\"Accuracy: {(cm[0][0] + cm[1][1]) / np.sum(cm):.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1: {(2 * precision * recall) / (precision + recall):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.5\n",
      "Training set performance:\n",
      "Accuracy: 0.8547\n",
      "Precision: 0.8661\n",
      "Recall: 0.8546\n",
      "F1: 0.8603\n",
      "Validation set performance:\n",
      "Accuracy: 0.8022\n",
      "Precision: 0.8175\n",
      "Recall: 0.8011\n",
      "F1: 0.8092\n",
      "\n",
      "Alpha: 1.0\n",
      "Training set performance:\n",
      "Accuracy: 0.8526\n",
      "Precision: 0.8625\n",
      "Recall: 0.8548\n",
      "F1: 0.8586\n",
      "Validation set performance:\n",
      "Accuracy: 0.8043\n",
      "Precision: 0.8185\n",
      "Recall: 0.8048\n",
      "F1: 0.8116\n",
      "\n",
      "Alpha: 1.5\n",
      "Training set performance:\n",
      "Accuracy: 0.8509\n",
      "Precision: 0.8607\n",
      "Recall: 0.8533\n",
      "F1: 0.8570\n",
      "Validation set performance:\n",
      "Accuracy: 0.8055\n",
      "Precision: 0.8195\n",
      "Recall: 0.8061\n",
      "F1: 0.8128\n",
      "\n",
      "Alpha: 2.0\n",
      "Training set performance:\n",
      "Accuracy: 0.8498\n",
      "Precision: 0.8591\n",
      "Recall: 0.8530\n",
      "F1: 0.8560\n",
      "Validation set performance:\n",
      "Accuracy: 0.8055\n",
      "Precision: 0.8202\n",
      "Recall: 0.8051\n",
      "F1: 0.8126\n",
      "\n",
      "Alpha: 2.5\n",
      "Training set performance:\n",
      "Accuracy: 0.8484\n",
      "Precision: 0.8573\n",
      "Recall: 0.8524\n",
      "F1: 0.8549\n",
      "Validation set performance:\n",
      "Accuracy: 0.8055\n",
      "Precision: 0.8198\n",
      "Recall: 0.8058\n",
      "F1: 0.8127\n",
      "\n",
      "Alpha: 3.0\n",
      "Training set performance:\n",
      "Accuracy: 0.8474\n",
      "Precision: 0.8556\n",
      "Recall: 0.8525\n",
      "F1: 0.8541\n",
      "Validation set performance:\n",
      "Accuracy: 0.8046\n",
      "Precision: 0.8184\n",
      "Recall: 0.8058\n",
      "F1: 0.8120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0.5, 3.5, 0.5)\n",
    "classifiers = {}\n",
    "for a in alphas:\n",
    "    print(f\"Alpha: {a}\")\n",
    "    classifier = MultinomialNB(alpha=a)\n",
    "    classifier.fit(train_X, train_Y)\n",
    "    classifiers[a] = classifier\n",
    "    print(f\"Training set performance:\")\n",
    "    train_pred = classifier.predict(train_X)\n",
    "    train_cm = confusion_matrix(train_Y, train_pred)\n",
    "    evaluate_confusion_matrix(train_cm)\n",
    "    print(f\"Validation set performance:\")\n",
    "    val_pred = classifier.predict(val_X)\n",
    "    val_cm = confusion_matrix(val_Y, val_pred)\n",
    "    evaluate_confusion_matrix(val_cm)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the classifier with smoothing parameter $ \\alpha = 1.5 $ performs the best on validation set,\n",
    "we will evaluate it on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of NB with alpha = 1.5\n",
      "Accuracy: 0.7961\n",
      "Precision: 0.8077\n",
      "Recall: 0.8012\n",
      "F1: 0.8044\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 1.5\n",
    "print(f\"Evaluation of NB with alpha = {ALPHA}\")\n",
    "test_pred = classifiers[ALPHA].predict(test_X)\n",
    "test_cm = confusion_matrix(test_Y, test_pred)\n",
    "evaluate_confusion_matrix(test_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
