\documentclass[11pt]{article}

\usepackage{authblk}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{biblatex}

\bibliography{ref.bib}

\newcommand{\email}[1]{\texttt{\href{mailto:#1}{#1}}}

\title{COMP4420 Project Proposal: Sarcasm Detection}
\author{
    Bui, Nam \\
    \email{nam\_bui@student.uml.edu}
    % \and
    % Put names here
}

\begin{document}

\maketitle

\section{Introduction}

% Sentiment analysis

% The difficulty of determining sarcasm
% e.g. sentence in one context may be sarcastic while in another context may be serious

\section{Dataset}

The dataset used will be a collection of
tagged newspaper headlines \cite{misra2023Sarcasm}.

% Structure of data in dataset
% When and how dataset was collected

% Headlines collected from TheOnion (sarcastic) and Huffington Post (not-sarcastic)
% Headlines from sources do not intersect (self-contained)
% Limitation of dataset: Only two news sources
% Additionally, sarcastic headlines from TheOnion are obviously sarcastic,
% so more subtle sarcasm is not captured in the dataset

\section{Evaluation Method}

% Will be interesting to do some EDA
Since the dataset was created in 2016,
during a period of political turmoil,
there may be some bias in the data.
It will be interesting to see what words
have the highest correlation with a sarcastic headline.
% News tends to have a lot of named entities,
% So it may be useful to have some NER
Additionally, news headlines usually have a lot of proper nouns,
so it may help to use named entity recognition when encoding the headlines.

% Models to test:
% "Traditional" models like Naive Bayes (IIRC I got >80% acc w/ NB and one-hot encoding in 2020)
% Context-independent NN like DAN
% Context-sensitive RNN w/ attention has been shown to perform well on this task
Sentiment analysis is a core natural language processing task
so there is a lot of data available on what types of models work.
We plan on using several for this project.
Naive Bayes classifiers are lightweight models that have
traditionally been used in sentiment analysis.
Deep averaging networks are able to leverage the universal approximation properties
of neural networks, but are lightweight since they don't capture context.
In recent years, recurrent neural networks have gained popularity
due to their ability to capture context with the attention mechanism.
Since news headlines are often one or two sentences,
there is not much need to capture long distance dependencies.

% Since it's classification, F1 is a decent metric
Since the task is a binary classification task,
precision, recall, and F1 are good metrics to use.
Accuracy will also be used to compare findings
to results from Misra et al. \cite{misra2023Sarcasm}.

\printbibliography

\end{document}