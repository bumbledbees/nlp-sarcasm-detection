{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from common import *\n",
    "from lstm import LSTMBinaryClassifier, calc_loss\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "TRAIN_FN = \"train_tokenized.json\"\n",
    "VAL_FN = \"val_tokenized.json\"\n",
    "TEST_FN = \"test_tokenized.json\"\n",
    "W2I_FN = \"word2ind.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(DATA_DIR + TRAIN_FN, lines=True)\n",
    "val_df = pd.read_json(DATA_DIR + VAL_FN, lines=True)\n",
    "with open(DATA_DIR + W2I_FN, \"r\") as fin:\n",
    "    w2i = json.load(fin)\n",
    "train_df[\"tokens\"] = train_df[\"tokens\"].apply(lambda s: vectorize_tokens(s, w2i))\n",
    "val_df[\"tokens\"] = val_df[\"tokens\"].apply(lambda s: vectorize_tokens(s, w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class NewspaperSarcasmDataset(Dataset):\n",
    "    def __init__(self, tokens: list[NDArray], labels: list[int], w2i: defaultdict[str, int]):\n",
    "        super().__init__()\n",
    "        tokens = [torch.LongTensor(tk) for tk in tokens]\n",
    "        self.tokens = pad_sequence(tokens, batch_first=True, padding_value=w2i[TK_END])\n",
    "        self.lengths = torch.LongTensor([len(tk) for tk in tokens])\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.tokens[i], self.lengths[i], self.labels[i]\n",
    "\n",
    "train_set = NewspaperSarcasmDataset(train_df[\"tokens\"], train_df[\"is_sarcastic\"], w2i)\n",
    "val_set = NewspaperSarcasmDataset(val_df[\"tokens\"], val_df[\"is_sarcastic\"], w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0660, -0.3232],\n",
      "        [ 0.0660, -0.3232],\n",
      "        [ 0.0660, -0.3232],\n",
      "        [ 0.0823, -0.3310],\n",
      "        [ 0.0660, -0.3232]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on forward pass\n",
    "inputs, lengths, labels = train_set[0:5]\n",
    "model = LSTMBinaryClassifier(len(w2i), 100, [64,10])\n",
    "output, _ = model.forward(inputs, lengths)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 100\n",
    "HIDDEN_SIZE = [64, 10]\n",
    "clip_value = 4.0  \n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 150\n",
    "MIN_EPOCHS = 25\n",
    "EARLY_STOP_THRESHOLD = 0.2\n",
    "NOISE_SD = 1e-3\n",
    "ALPHA = 5e-3\n",
    "LR_GAMMA = 0.98\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"\"\n",
    "CHKPT_INTERVAL = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMBinaryClassifier(len(w2i), EMBED_SIZE, HIDDEN_SIZE).to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=ALPHA)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      " train_loss=0.6898, val_loss=0.6912\n",
      " train_acc=0.5324, val_acc=0.5328\n",
      "epoch 1:\n",
      " train_loss=0.6894, val_loss=0.6902\n",
      " train_acc=0.5347, val_acc=0.5347\n",
      "epoch 2:\n",
      " train_loss=0.6896, val_loss=0.6884\n",
      " train_acc=0.5334, val_acc=0.5354\n",
      "epoch 3:\n",
      " train_loss=0.6886, val_loss=0.6906\n",
      " train_acc=0.5363, val_acc=0.5317\n",
      "epoch 4:\n",
      " train_loss=0.6881, val_loss=0.6893\n",
      " train_acc=0.5371, val_acc=0.5350\n",
      "epoch 5:\n",
      " train_loss=0.6879, val_loss=0.6900\n",
      " train_acc=0.5374, val_acc=0.5336\n",
      "epoch 6:\n",
      " train_loss=0.6886, val_loss=0.6892\n",
      " train_acc=0.5360, val_acc=0.5361\n",
      "epoch 7:\n",
      " train_loss=0.6877, val_loss=0.6873\n",
      " train_acc=0.5373, val_acc=0.5363\n",
      "epoch 8:\n",
      " train_loss=0.6859, val_loss=0.6903\n",
      " train_acc=0.5388, val_acc=0.5349\n",
      "epoch 9:\n",
      " train_loss=0.6866, val_loss=0.6880\n",
      " train_acc=0.5379, val_acc=0.5349\n",
      "epoch 10:\n",
      " train_loss=0.6853, val_loss=0.6880\n",
      " train_acc=0.5394, val_acc=0.5378\n",
      "epoch 11:\n",
      " train_loss=0.6857, val_loss=0.6901\n",
      " train_acc=0.5387, val_acc=0.5338\n",
      "epoch 12:\n",
      " train_loss=0.6861, val_loss=0.6891\n",
      " train_acc=0.5385, val_acc=0.5342\n",
      "epoch 13:\n",
      " train_loss=0.6862, val_loss=0.6895\n",
      " train_acc=0.5376, val_acc=0.5335\n",
      "epoch 14:\n",
      " train_loss=0.6852, val_loss=0.6871\n",
      " train_acc=0.5384, val_acc=0.5380\n",
      "epoch 15:\n",
      " train_loss=0.6856, val_loss=0.6865\n",
      " train_acc=0.5384, val_acc=0.5366\n",
      "epoch 16:\n",
      " train_loss=0.6848, val_loss=0.6878\n",
      " train_acc=0.5386, val_acc=0.5345\n",
      "epoch 17:\n",
      " train_loss=0.6853, val_loss=0.6856\n",
      " train_acc=0.5384, val_acc=0.5382\n",
      "epoch 18:\n",
      " train_loss=0.6851, val_loss=0.6879\n",
      " train_acc=0.5382, val_acc=0.5368\n",
      "epoch 19:\n",
      " train_loss=0.6843, val_loss=0.6860\n",
      " train_acc=0.5371, val_acc=0.5373\n",
      "epoch 20:\n",
      " train_loss=0.6840, val_loss=0.6851\n",
      " train_acc=0.5395, val_acc=0.5398\n",
      "epoch 21:\n",
      " train_loss=0.6843, val_loss=0.6853\n",
      " train_acc=0.5390, val_acc=0.5382\n",
      "epoch 22:\n",
      " train_loss=0.6848, val_loss=0.6863\n",
      " train_acc=0.5389, val_acc=0.5370\n",
      "epoch 23:\n",
      " train_loss=0.6843, val_loss=0.6851\n",
      " train_acc=0.5393, val_acc=0.5385\n",
      "epoch 24:\n",
      " train_loss=0.6834, val_loss=0.6844\n",
      " train_acc=0.5404, val_acc=0.5398\n",
      "epoch 25:\n",
      " train_loss=0.6822, val_loss=0.6857\n",
      " train_acc=0.5418, val_acc=0.5384\n",
      "epoch 26:\n",
      " train_loss=0.6812, val_loss=0.6866\n",
      " train_acc=0.5414, val_acc=0.5399\n",
      "epoch 27:\n",
      " train_loss=0.6823, val_loss=0.6854\n",
      " train_acc=0.5409, val_acc=0.5375\n",
      "epoch 28:\n",
      " train_loss=0.6824, val_loss=0.6846\n",
      " train_acc=0.5402, val_acc=0.5377\n",
      "epoch 29:\n",
      " train_loss=0.6837, val_loss=0.6835\n",
      " train_acc=0.5394, val_acc=0.5405\n",
      "epoch 30:\n",
      " train_loss=0.6808, val_loss=0.6828\n",
      " train_acc=0.5419, val_acc=0.5398\n",
      "epoch 31:\n",
      " train_loss=0.6826, val_loss=0.6866\n",
      " train_acc=0.5405, val_acc=0.5357\n",
      "epoch 32:\n",
      " train_loss=0.6817, val_loss=0.6852\n",
      " train_acc=0.5410, val_acc=0.5384\n",
      "epoch 33:\n",
      " train_loss=0.6819, val_loss=0.6865\n",
      " train_acc=0.5411, val_acc=0.5371\n",
      "epoch 34:\n",
      " train_loss=0.6815, val_loss=0.6870\n",
      " train_acc=0.5405, val_acc=0.5373\n",
      "epoch 35:\n",
      " train_loss=0.6827, val_loss=0.6855\n",
      " train_acc=0.5414, val_acc=0.5396\n",
      "epoch 36:\n",
      " train_loss=0.6815, val_loss=0.6874\n",
      " train_acc=0.5406, val_acc=0.5357\n",
      "epoch 37:\n",
      " train_loss=0.6816, val_loss=0.6845\n",
      " train_acc=0.5402, val_acc=0.5385\n",
      "epoch 38:\n",
      " train_loss=0.6816, val_loss=0.6826\n",
      " train_acc=0.5410, val_acc=0.5399\n",
      "epoch 39:\n",
      " train_loss=0.6806, val_loss=0.6846\n",
      " train_acc=0.5418, val_acc=0.5394\n",
      "epoch 40:\n",
      " train_loss=0.6808, val_loss=0.6847\n",
      " train_acc=0.5407, val_acc=0.5391\n",
      "epoch 41:\n",
      " train_loss=0.6804, val_loss=0.6873\n",
      " train_acc=0.5414, val_acc=0.5363\n",
      "epoch 42:\n",
      " train_loss=0.6812, val_loss=0.6857\n",
      " train_acc=0.5411, val_acc=0.5373\n",
      "epoch 43:\n",
      " train_loss=0.6800, val_loss=0.6835\n",
      " train_acc=0.5423, val_acc=0.5406\n",
      "epoch 44:\n",
      " train_loss=0.6788, val_loss=0.6841\n",
      " train_acc=0.5422, val_acc=0.5394\n",
      "epoch 45:\n",
      " train_loss=0.6816, val_loss=0.6832\n",
      " train_acc=0.5415, val_acc=0.5391\n",
      "epoch 46:\n",
      " train_loss=0.6812, val_loss=0.6804\n",
      " train_acc=0.5409, val_acc=0.5408\n",
      "epoch 47:\n",
      " train_loss=0.6810, val_loss=0.6826\n",
      " train_acc=0.5407, val_acc=0.5410\n",
      "epoch 48:\n",
      " train_loss=0.6808, val_loss=0.6807\n",
      " train_acc=0.5421, val_acc=0.5413\n",
      "epoch 49:\n",
      " train_loss=0.6804, val_loss=0.6838\n",
      " train_acc=0.5416, val_acc=0.5408\n",
      "epoch 50:\n",
      " train_loss=0.6807, val_loss=0.6829\n",
      " train_acc=0.5402, val_acc=0.5377\n",
      "epoch 51:\n",
      " train_loss=0.6807, val_loss=0.6807\n",
      " train_acc=0.5419, val_acc=0.5434\n",
      "epoch 52:\n",
      " train_loss=0.6810, val_loss=0.6822\n",
      " train_acc=0.5415, val_acc=0.5415\n",
      "epoch 53:\n",
      " train_loss=0.6798, val_loss=0.6806\n",
      " train_acc=0.5417, val_acc=0.5410\n",
      "epoch 54:\n",
      " train_loss=0.6800, val_loss=0.6824\n",
      " train_acc=0.5425, val_acc=0.5403\n",
      "epoch 55:\n",
      " train_loss=0.6795, val_loss=0.6821\n",
      " train_acc=0.5421, val_acc=0.5405\n",
      "epoch 56:\n",
      " train_loss=0.6799, val_loss=0.6813\n",
      " train_acc=0.5419, val_acc=0.5406\n",
      "epoch 57:\n",
      " train_loss=0.6793, val_loss=0.6855\n",
      " train_acc=0.5429, val_acc=0.5373\n",
      "epoch 58:\n",
      " train_loss=0.6796, val_loss=0.6846\n",
      " train_acc=0.5424, val_acc=0.5377\n",
      "epoch 59:\n",
      " train_loss=0.6808, val_loss=0.6830\n",
      " train_acc=0.5415, val_acc=0.5391\n",
      "epoch 60:\n",
      " train_loss=0.6790, val_loss=0.6824\n",
      " train_acc=0.5413, val_acc=0.5398\n",
      "epoch 61:\n",
      " train_loss=0.6804, val_loss=0.6833\n",
      " train_acc=0.5415, val_acc=0.5382\n",
      "epoch 62:\n",
      " train_loss=0.6782, val_loss=0.6849\n",
      " train_acc=0.5431, val_acc=0.5378\n",
      "epoch 63:\n",
      " train_loss=0.6794, val_loss=0.6822\n",
      " train_acc=0.5423, val_acc=0.5425\n",
      "epoch 64:\n",
      " train_loss=0.6792, val_loss=0.6837\n",
      " train_acc=0.5421, val_acc=0.5389\n",
      "epoch 65:\n",
      " train_loss=0.6792, val_loss=0.6830\n",
      " train_acc=0.5425, val_acc=0.5399\n",
      "epoch 66:\n",
      " train_loss=0.6797, val_loss=0.6810\n",
      " train_acc=0.5418, val_acc=0.5411\n",
      "epoch 67:\n",
      " train_loss=0.6777, val_loss=0.6843\n",
      " train_acc=0.5431, val_acc=0.5401\n",
      "epoch 68:\n",
      " train_loss=0.6792, val_loss=0.6803\n",
      " train_acc=0.5426, val_acc=0.5408\n",
      "epoch 69:\n",
      " train_loss=0.6782, val_loss=0.6836\n",
      " train_acc=0.5437, val_acc=0.5396\n",
      "epoch 70:\n",
      " train_loss=0.6788, val_loss=0.6820\n",
      " train_acc=0.5431, val_acc=0.5405\n",
      "epoch 71:\n",
      " train_loss=0.6784, val_loss=0.6813\n",
      " train_acc=0.5432, val_acc=0.5411\n",
      "epoch 72:\n",
      " train_loss=0.6795, val_loss=0.6798\n",
      " train_acc=0.5413, val_acc=0.5405\n",
      "epoch 73:\n",
      " train_loss=0.6789, val_loss=0.6829\n",
      " train_acc=0.5425, val_acc=0.5401\n",
      "epoch 74:\n",
      " train_loss=0.6794, val_loss=0.6832\n",
      " train_acc=0.5417, val_acc=0.5391\n",
      "epoch 75:\n",
      " train_loss=0.6776, val_loss=0.6814\n",
      " train_acc=0.5435, val_acc=0.5420\n",
      "epoch 76:\n",
      " train_loss=0.6792, val_loss=0.6827\n",
      " train_acc=0.5428, val_acc=0.5392\n",
      "epoch 77:\n",
      " train_loss=0.6800, val_loss=0.6822\n",
      " train_acc=0.5403, val_acc=0.5401\n",
      "epoch 78:\n",
      " train_loss=0.6788, val_loss=0.6799\n",
      " train_acc=0.5427, val_acc=0.5434\n",
      "epoch 79:\n",
      " train_loss=0.6778, val_loss=0.6823\n",
      " train_acc=0.5436, val_acc=0.5401\n",
      "epoch 80:\n",
      " train_loss=0.6791, val_loss=0.6826\n",
      " train_acc=0.5426, val_acc=0.5405\n",
      "epoch 81:\n",
      " train_loss=0.6784, val_loss=0.6815\n",
      " train_acc=0.5433, val_acc=0.5410\n",
      "epoch 82:\n",
      " train_loss=0.6781, val_loss=0.6791\n",
      " train_acc=0.5437, val_acc=0.5434\n",
      "epoch 83:\n",
      " train_loss=0.6781, val_loss=0.6817\n",
      " train_acc=0.5423, val_acc=0.5408\n",
      "epoch 84:\n",
      " train_loss=0.6775, val_loss=0.6833\n",
      " train_acc=0.5435, val_acc=0.5399\n",
      "epoch 85:\n",
      " train_loss=0.6775, val_loss=0.6800\n",
      " train_acc=0.5434, val_acc=0.5422\n",
      "epoch 86:\n",
      " train_loss=0.6768, val_loss=0.6848\n",
      " train_acc=0.5435, val_acc=0.5391\n",
      "epoch 87:\n",
      " train_loss=0.6792, val_loss=0.6848\n",
      " train_acc=0.5418, val_acc=0.5391\n",
      "epoch 88:\n",
      " train_loss=0.6775, val_loss=0.6807\n",
      " train_acc=0.5443, val_acc=0.5420\n",
      "epoch 89:\n",
      " train_loss=0.6787, val_loss=0.6836\n",
      " train_acc=0.5420, val_acc=0.5391\n",
      "epoch 90:\n",
      " train_loss=0.6788, val_loss=0.6815\n",
      " train_acc=0.5424, val_acc=0.5405\n",
      "epoch 91:\n",
      " train_loss=0.6774, val_loss=0.6811\n",
      " train_acc=0.5440, val_acc=0.5411\n",
      "epoch 92:\n",
      " train_loss=0.6774, val_loss=0.6819\n",
      " train_acc=0.5434, val_acc=0.5401\n",
      "epoch 93:\n",
      " train_loss=0.6786, val_loss=0.6818\n",
      " train_acc=0.5417, val_acc=0.5411\n",
      "epoch 94:\n",
      " train_loss=0.6779, val_loss=0.6839\n",
      " train_acc=0.5430, val_acc=0.5391\n",
      "epoch 95:\n",
      " train_loss=0.6791, val_loss=0.6832\n",
      " train_acc=0.5414, val_acc=0.5391\n",
      "epoch 96:\n",
      " train_loss=0.6786, val_loss=0.6827\n",
      " train_acc=0.5421, val_acc=0.5403\n",
      "epoch 97:\n",
      " train_loss=0.6779, val_loss=0.6824\n",
      " train_acc=0.5437, val_acc=0.5399\n",
      "epoch 98:\n",
      " train_loss=0.6786, val_loss=0.6807\n",
      " train_acc=0.5426, val_acc=0.5411\n",
      "epoch 99:\n",
      " train_loss=0.6764, val_loss=0.6809\n",
      " train_acc=0.5439, val_acc=0.5406\n",
      "epoch 100:\n",
      " train_loss=0.6773, val_loss=0.6829\n",
      " train_acc=0.5435, val_acc=0.5399\n",
      "epoch 101:\n",
      " train_loss=0.6777, val_loss=0.6811\n",
      " train_acc=0.5431, val_acc=0.5391\n",
      "epoch 102:\n",
      " train_loss=0.6786, val_loss=0.6782\n",
      " train_acc=0.5423, val_acc=0.5446\n",
      "epoch 103:\n",
      " train_loss=0.6766, val_loss=0.6855\n",
      " train_acc=0.5440, val_acc=0.5405\n",
      "epoch 104:\n",
      " train_loss=0.6779, val_loss=0.6806\n",
      " train_acc=0.5426, val_acc=0.5422\n",
      "epoch 105:\n",
      " train_loss=0.6774, val_loss=0.6847\n",
      " train_acc=0.5425, val_acc=0.5387\n",
      "epoch 106:\n",
      " train_loss=0.6775, val_loss=0.6826\n",
      " train_acc=0.5437, val_acc=0.5405\n",
      "epoch 107:\n",
      " train_loss=0.6785, val_loss=0.6807\n",
      " train_acc=0.5423, val_acc=0.5399\n",
      "epoch 108:\n",
      " train_loss=0.6772, val_loss=0.6817\n",
      " train_acc=0.5440, val_acc=0.5411\n",
      "epoch 109:\n",
      " train_loss=0.6772, val_loss=0.6813\n",
      " train_acc=0.5441, val_acc=0.5420\n",
      "epoch 110:\n",
      " train_loss=0.6779, val_loss=0.6844\n",
      " train_acc=0.5432, val_acc=0.5398\n",
      "epoch 111:\n",
      " train_loss=0.6766, val_loss=0.6818\n",
      " train_acc=0.5439, val_acc=0.5403\n",
      "epoch 112:\n",
      " train_loss=0.6779, val_loss=0.6804\n",
      " train_acc=0.5419, val_acc=0.5422\n",
      "epoch 113:\n",
      " train_loss=0.6767, val_loss=0.6819\n",
      " train_acc=0.5435, val_acc=0.5413\n",
      "epoch 114:\n",
      " train_loss=0.6764, val_loss=0.6828\n",
      " train_acc=0.5433, val_acc=0.5399\n",
      "epoch 115:\n",
      " train_loss=0.6779, val_loss=0.6810\n",
      " train_acc=0.5436, val_acc=0.5406\n",
      "epoch 116:\n",
      " train_loss=0.6775, val_loss=0.6804\n",
      " train_acc=0.5434, val_acc=0.5417\n",
      "epoch 117:\n",
      " train_loss=0.6768, val_loss=0.6825\n",
      " train_acc=0.5443, val_acc=0.5410\n",
      "epoch 118:\n",
      " train_loss=0.6779, val_loss=0.6816\n",
      " train_acc=0.5420, val_acc=0.5429\n",
      "epoch 119:\n",
      " train_loss=0.6774, val_loss=0.6789\n",
      " train_acc=0.5430, val_acc=0.5432\n",
      "epoch 120:\n",
      " train_loss=0.6768, val_loss=0.6817\n",
      " train_acc=0.5436, val_acc=0.5411\n",
      "epoch 121:\n",
      " train_loss=0.6775, val_loss=0.6794\n",
      " train_acc=0.5428, val_acc=0.5417\n",
      "epoch 122:\n",
      " train_loss=0.6769, val_loss=0.6801\n",
      " train_acc=0.5444, val_acc=0.5422\n",
      "epoch 123:\n",
      " train_loss=0.6774, val_loss=0.6829\n",
      " train_acc=0.5435, val_acc=0.5396\n",
      "epoch 124:\n",
      " train_loss=0.6774, val_loss=0.6825\n",
      " train_acc=0.5428, val_acc=0.5413\n",
      "epoch 125:\n",
      " train_loss=0.6763, val_loss=0.6818\n",
      " train_acc=0.5434, val_acc=0.5399\n",
      "epoch 126:\n",
      " train_loss=0.6784, val_loss=0.6820\n",
      " train_acc=0.5421, val_acc=0.5401\n",
      "epoch 127:\n",
      " train_loss=0.6754, val_loss=0.6799\n",
      " train_acc=0.5442, val_acc=0.5415\n",
      "epoch 128:\n",
      " train_loss=0.6772, val_loss=0.6831\n",
      " train_acc=0.5429, val_acc=0.5401\n",
      "epoch 129:\n",
      " train_loss=0.6764, val_loss=0.6792\n",
      " train_acc=0.5440, val_acc=0.5422\n",
      "epoch 130:\n",
      " train_loss=0.6776, val_loss=0.6827\n",
      " train_acc=0.5436, val_acc=0.5399\n",
      "epoch 131:\n",
      " train_loss=0.6781, val_loss=0.6827\n",
      " train_acc=0.5424, val_acc=0.5401\n",
      "epoch 132:\n",
      " train_loss=0.6785, val_loss=0.6817\n",
      " train_acc=0.5420, val_acc=0.5396\n",
      "epoch 133:\n",
      " train_loss=0.6764, val_loss=0.6787\n",
      " train_acc=0.5440, val_acc=0.5429\n",
      "epoch 134:\n",
      " train_loss=0.6759, val_loss=0.6837\n",
      " train_acc=0.5447, val_acc=0.5387\n",
      "epoch 135:\n",
      " train_loss=0.6761, val_loss=0.6837\n",
      " train_acc=0.5436, val_acc=0.5410\n",
      "epoch 136:\n",
      " train_loss=0.6768, val_loss=0.6821\n",
      " train_acc=0.5435, val_acc=0.5418\n",
      "epoch 137:\n",
      " train_loss=0.6769, val_loss=0.6830\n",
      " train_acc=0.5439, val_acc=0.5385\n",
      "epoch 138:\n",
      " train_loss=0.6760, val_loss=0.6809\n",
      " train_acc=0.5442, val_acc=0.5422\n",
      "epoch 139:\n",
      " train_loss=0.6761, val_loss=0.6835\n",
      " train_acc=0.5443, val_acc=0.5425\n",
      "epoch 140:\n",
      " train_loss=0.6757, val_loss=0.6806\n",
      " train_acc=0.5443, val_acc=0.5425\n",
      "epoch 141:\n",
      " train_loss=0.6789, val_loss=0.6816\n",
      " train_acc=0.5417, val_acc=0.5427\n",
      "epoch 142:\n",
      " train_loss=0.6787, val_loss=0.6804\n",
      " train_acc=0.5437, val_acc=0.5408\n",
      "epoch 143:\n",
      " train_loss=0.6762, val_loss=0.6797\n",
      " train_acc=0.5433, val_acc=0.5443\n",
      "epoch 144:\n",
      " train_loss=0.6769, val_loss=0.6802\n",
      " train_acc=0.5439, val_acc=0.5418\n",
      "epoch 145:\n",
      " train_loss=0.6768, val_loss=0.6797\n",
      " train_acc=0.5437, val_acc=0.5425\n",
      "epoch 146:\n",
      " train_loss=0.6764, val_loss=0.6826\n",
      " train_acc=0.5432, val_acc=0.5380\n",
      "epoch 147:\n",
      " train_loss=0.6773, val_loss=0.6807\n",
      " train_acc=0.5424, val_acc=0.5403\n",
      "epoch 148:\n",
      " train_loss=0.6780, val_loss=0.6812\n",
      " train_acc=0.5431, val_acc=0.5406\n",
      "epoch 149:\n",
      " train_loss=0.6779, val_loss=0.6824\n",
      " train_acc=0.5434, val_acc=0.5403\n"
     ]
    }
   ],
   "source": [
    "min_loss = np.inf\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"epoch {epoch}:\")\n",
    "\n",
    "    # Train over training set\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    for sid, (sent, lengths, labels) in enumerate(train_loader):\n",
    "        # Calculate model error and  propogate loss\n",
    "        loss, corr = calc_loss(model, sent, lengths, labels, criterion, device=device)\n",
    "        running_loss += loss.item()\n",
    "        correct += corr.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Regularization: gradient clipping and noisy gradients\n",
    "        clip_grad_norm_(model.parameters(), clip_value * BATCH_SIZE)\n",
    "        for layer in model.parameters():\n",
    "            layer.grad += torch.randn_like(layer.grad) * NOISE_SD * BATCH_SIZE\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate over validation set\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sid, (sent, lengths, labels) in enumerate(val_loader):\n",
    "            loss, corr = calc_loss(model, sent, lengths, labels, criterion, device=device)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += corr.item()\n",
    "\n",
    "    # Print loss and accuracy\n",
    "    train_loss = running_loss / len(train_set)\n",
    "    val_loss = val_loss / len(val_set)\n",
    "    print(f\" train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "    print(\n",
    "        f\" train_acc={correct / len(train_set):.4f}, val_acc={val_correct/len(val_set):.4f}\"\n",
    "    )\n",
    "\n",
    "    # Early stoppage:\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "    elif epoch > MIN_EPOCHS and val_loss > min_loss + EARLY_STOP_THRESHOLD:\n",
    "        torch.save(model.state_dict(), f\"{MODEL_OUTPUT_DIR}SD_LSTM_ep{epoch}.pt\")\n",
    "        print(f\"Early stopping at epoch {epoch}.\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch % CHKPT_INTERVAL == 0 and epoch > 0:\n",
    "        torch.save(model.state_dict(), f\"{MODEL_OUTPUT_DIR}SD_LSTM_ep{epoch}.pt\")\n",
    "    \n",
    "    # Update LR\n",
    "    scheduler.step()\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), f\"{MODEL_OUTPUT_DIR}SD_LSTM_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2864 samples tested.\n",
      "test_loss=0.6827, test_acc=0.5356\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_json(DATA_DIR + TEST_FN, lines=True)\n",
    "test_df[\"tokens\"] = test_df[\"tokens\"].apply(lambda s: vectorize_tokens(s, w2i))\n",
    "test_set = NewspaperSarcasmDataset(test_df[\"tokens\"], test_df[\"is_sarcastic\"], w2i)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "test_loss = test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for sid, (sent, lengths, labels) in enumerate(test_loader):\n",
    "        loss, corr = calc_loss(model, sent, lengths, labels, criterion, device=device)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += corr\n",
    "print(f\"{len(test_set)} samples tested.\")\n",
    "print(f\"test_loss={test_loss/len(test_set):.4f}, test_acc={test_correct/len(test_set):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
